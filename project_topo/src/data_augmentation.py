import pandas as pd
from transformers import MarianMTModel, MarianTokenizer, AutoModel, AutoTokenizer
import random
import py_vncorenlp
import torch
import os
import numpy as np
from ripser import ripser
from persim import plot_diagrams, wasserstein
from sklearn.preprocessing import StandardScaler

# Khá»Ÿi táº¡o VnCoreNLP
py_vncorenlp.download_model(save_dir="C:/VnCoreNLP")
annotator = py_vncorenlp.VnCoreNLP(annotators=["wseg"], save_dir="C:/VnCoreNLP")

# Tá»« Ä‘iá»ƒn Ä‘á»“ng nghÄ©a
synonyms_dict = {
    "Ä‘áº¹p": ["xinh", "lá»™ng láº«y", "má»¹ miá»u"],
    "tuyá»‡t vá»i": ["xuáº¥t sáº¯c", "hoÃ n háº£o", "tuyá»‡t diá»‡u"],
    "tá»‘t": ["tuyá»‡t", "ok", "hÃ i lÃ²ng"],
    "nhanh": ["mau", "láº¹", "tá»‘c Ä‘á»™"],
    "á»•n": ["tá»‘t", "Ä‘Æ°á»£c", "hÃ i lÃ²ng"],
}

# HÃ m láº¥y embedding tá»« PhoBERT
def get_phobert_embeddings(texts, tokenizer, model, device, max_len=128):
    model.eval()
    embeddings = []
    for text in texts:
        inputs = tokenizer.encode_plus(
            text, max_length=max_len, padding="max_length", truncation=True, return_tensors="pt"
        )
        input_ids = inputs["input_ids"].to(device)
        attention_mask = inputs["attention_mask"].to(device)
        with torch.no_grad():
            outputs = model(input_ids, attention_mask=attention_mask)
            embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()  # Láº¥y [CLS] token
        embeddings.append(embedding[0])
    return np.array(embeddings)

# HÃ m tÃ­nh persistence diagrams vÃ  chá»n bÃ¬nh luáº­n cáº§n tÄƒng cÆ°á»ng
def select_comments_for_augmentation(embeddings, threshold=0.5):
    # Chuáº©n hÃ³a dá»¯ liá»‡u
    scaler = StandardScaler()
    embeddings_scaled = scaler.fit_transform(embeddings)
    
    # TÃ­nh persistence diagrams vá»›i Ripser
    diagrams = ripser(embeddings_scaled, maxdim=1)['dgms']
    
    # TÃ­nh khoáº£ng cÃ¡ch Wasserstein giá»¯a cÃ¡c Ä‘iá»ƒm
    distances = []
    for i in range(len(embeddings)):
        dist = wasserstein(diagrams[0], diagrams[0], i, i)  # So sÃ¡nh vá»›i chÃ­nh nÃ³ (placeholder)
        distances.append(dist)
    
    # Chá»n cÃ¡c bÃ¬nh luáº­n cÃ³ khoáº£ng cÃ¡ch topological lá»›n (Ã­t Ä‘áº¡i diá»‡n)
    selected_indices = [i for i, dist in enumerate(distances) if dist > threshold]
    return selected_indices

# HÃ m thay tháº¿ tá»« Ä‘á»“ng nghÄ©a
def synonym_replacement(comment):
    segmented_text = annotator.word_segment(comment)
    if segmented_text:
        words = segmented_text[0].split()
    else:
        words = comment.split()
    new_words = words.copy()
    for i, word in enumerate(words):
        if word in synonyms_dict and random.random() < 0.3:
            new_words[i] = random.choice(synonyms_dict[word])
    return " ".join(new_words)

# HÃ m tÄƒng cÆ°á»ng dá»¯ liá»‡u vá»›i TDA
def augment_data(data, tokenizer, model, device):
    augmented_data = []
    comments = data["comment"].values
    
    print("ðŸš€ Táº¡o embedding tá»« PhoBERT...")
    embeddings = get_phobert_embeddings(comments, tokenizer, model, device)
    
    print("ðŸ” PhÃ¢n tÃ­ch topological vá»›i Ripser...")
    selected_indices = select_comments_for_augmentation(embeddings)
    
    print(f"âœ… ÄÃ£ chá»n {len(selected_indices)} bÃ¬nh luáº­n Ä‘á»ƒ tÄƒng cÆ°á»ng.")
    
    for idx, row in data.iterrows():
        comment = row["comment"]
        label = row["label"]
        rate = row["rate"]
        augmented_data.append({"comment": comment, "label": label, "rate": rate})
        
        # Chá»‰ tÄƒng cÆ°á»ng dá»¯ liá»‡u cho cÃ¡c bÃ¬nh luáº­n Ä‘Æ°á»£c chá»n bá»Ÿi TDA
        if idx in selected_indices:
            augmented_comment = synonym_replacement(comment)
            augmented_data.append({"comment": augmented_comment, "label": label, "rate": rate})
    
    return pd.DataFrame(augmented_data)

# Cháº¡y chÃ­nh
if __name__ == "__main__":
    try:
        # Load cáº¥u hÃ¬nh vÃ  tokenizer
        project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
        input_path = os.path.join(project_root, "data", "raw", "test_5k.csv")
        output_dir = os.path.join(project_root, "data", "processed")
        output_path = os.path.join(output_dir, "augmented_test_2k_with_tda.csv")

        print(f"ðŸ“„ Äang Ä‘á»c file tá»«: {input_path}")
        if not os.path.exists(input_path):
            raise FileNotFoundError(f"KhÃ´ng tÃ¬m tháº¥y file: {input_path}")

        data = pd.read_csv(input_path, usecols=["comment", "label", "rate"], on_bad_lines='skip')

        print("âœ… Äá»c file thÃ nh cÃ´ng, báº¯t Ä‘áº§u kiá»ƒm tra cáº¥u trÃºc...")
        required_columns = {"comment", "label", "rate"}
        if not required_columns.issubset(data.columns):
            raise ValueError("âŒ File CSV pháº£i cÃ³ Ä‘áº§y Ä‘á»§ cÃ¡c cá»™t: comment, label, rate")

        # Load PhoBERT Ä‘á»ƒ táº¡o embedding
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        tokenizer = AutoTokenizer.from_pretrained("vinai/phobert-base")
        model = AutoModel.from_pretrained("vinai/phobert-base").to(device)

        print("ðŸš€ Báº¯t Ä‘áº§u tÄƒng cÆ°á»ng dá»¯ liá»‡u vá»›i TDA...")
        augmented_data = augment_data(data, tokenizer, model, device)

        os.makedirs(output_dir, exist_ok=True)
        augmented_data.to_csv(output_path, index=False)

        print(f"âœ… TÄƒng cÆ°á»ng dá»¯ liá»‡u hoÃ n táº¥t. File Ä‘Ã£ lÆ°u táº¡i: {output_path}")
    except Exception as e:
        print(f"âŒ Lá»—i: {e}")