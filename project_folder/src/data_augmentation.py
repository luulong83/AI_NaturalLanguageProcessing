import pandas as pd
from transformers import MarianMTModel, MarianTokenizer
import random
import py_vncorenlp
import torch
import os

# Khá»Ÿi táº¡o VnCoreNLP
py_vncorenlp.download_model(save_dir="C:/VnCoreNLP")
annotator = py_vncorenlp.VnCoreNLP(annotators=["wseg"], save_dir="C:/VnCoreNLP")

# Tá»« Ä‘iá»ƒn Ä‘á»“ng nghÄ©a
synonyms_dict = {
    "Ä‘áº¹p": ["xinh", "lá»™ng láº«y", "má»¹ miá»u"],
    "tuyá»‡t vá»i": ["xuáº¥t sáº¯c", "hoÃ n háº£o", "tuyá»‡t diá»‡u"],
    "tá»‘t": ["tuyá»‡t", "ok", "hÃ i lÃ²ng"],
    "nhanh": ["mau", "láº¹", "tá»‘c Ä‘á»™"],
    "á»•n": ["tá»‘t", "Ä‘Æ°á»£c", "hÃ i lÃ²ng"],
    # ThÃªm cÃ¡c tá»« Ä‘á»“ng nghÄ©a khÃ¡c náº¿u cáº§n
}

# HÃ m thay tháº¿ tá»« Ä‘á»“ng nghÄ©a
def synonym_replacement(comment):
    segmented_text = annotator.word_segment(comment)
    if segmented_text:
        words = segmented_text[0].split()
    else:
        words = comment.split()
    new_words = words.copy()
    for i, word in enumerate(words):
        if word in synonyms_dict and random.random() < 0.3:
            new_words[i] = random.choice(synonyms_dict[word])
    return " ".join(new_words)

# HÃ m tÄƒng cÆ°á»ng dá»¯ liá»‡u
def augment_data(data):
    augmented_data = []
    for _, row in data.iterrows():
        comment = row["comment"]
        label = row["label"]
        rate = row["rate"]
        augmented_data.extend([
            {"comment": comment, "label": label, "rate": rate},
            {"comment": synonym_replacement(comment), "label": label, "rate": rate},
        ])
    return pd.DataFrame(augmented_data)

# Cháº¡y chÃ­nh
if __name__ == "__main__":
    try:
        # Láº¥y Ä‘Æ°á»ng dáº«n tuyá»‡t Ä‘á»‘i Ä‘áº¿n thÆ° má»¥c gá»‘c dá»± Ã¡n
        project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
        input_path = os.path.join(project_root, "data", "raw", "test_5k.csv")
        output_dir = os.path.join(project_root, "data", "processed")
        output_path = os.path.join(output_dir, "augmented_test_2k.csv")

        print(f"ğŸ“„ Äang Ä‘á»c file tá»«: {input_path}")
        if not os.path.exists(input_path):
            raise FileNotFoundError(f"KhÃ´ng tÃ¬m tháº¥y file: {input_path}")

        data = pd.read_csv(input_path, usecols=["comment", "label", "rate"], on_bad_lines='skip')

        print("âœ… Äá»c file thÃ nh cÃ´ng, báº¯t Ä‘áº§u kiá»ƒm tra cáº¥u trÃºc...")
        required_columns = {"comment", "label", "rate"}
        if not required_columns.issubset(data.columns):
            raise ValueError("âŒ File CSV pháº£i cÃ³ Ä‘áº§y Ä‘á»§ cÃ¡c cá»™t: comment, label, rate")

        print("ğŸš€ Cáº¥u trÃºc há»£p lá»‡, báº¯t Ä‘áº§u tÄƒng cÆ°á»ng dá»¯ liá»‡u...")
        augmented_data = augment_data(data)

        os.makedirs(output_dir, exist_ok=True)
        augmented_data.to_csv(output_path, index=False)

        print(f"âœ… TÄƒng cÆ°á»ng dá»¯ liá»‡u hoÃ n táº¥t. File Ä‘Ã£ lÆ°u táº¡i: {output_path}")
    except Exception as e:
        print(f"âŒ Lá»—i: {e}")
