from transformers import T5Tokenizer, T5ForConditionalGeneration
from underthesea import pos_tag, word_tokenize
import pandas as pd

# --------------------
# 1. H√†m chu·∫©n h√≥a vƒÉn b·∫£n (t·ª´ infer.py)
# --------------------
def chuan_hoa_van_ban(text, model_path="./vit5_chuanhoa_model"):
    try:
        tokenizer = T5Tokenizer.from_pretrained(model_path)
        model = T5ForConditionalGeneration.from_pretrained(model_path)
    except Exception as e:
        print(f"‚ùå L·ªói load m√¥ h√¨nh ho·∫∑c tokenizer: {e}")
        return None

    input_ids = tokenizer("chuan_hoa: " + text, return_tensors="pt").input_ids
    output_ids = model.generate(
        input_ids,
        max_length=128,
        num_beams=10,
        early_stopping=True,
        do_sample=False,
        no_repeat_ngram_size=3
    )
    output = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    return output

# --------------------
# 2. H√†m g√°n nh√£n t·ª´ lo·∫°i
# --------------------
def gan_nhan_tu_loai(sentence):
    try:
        # T√°ch t·ª´ b·∫±ng Underthesea
        tokenized_sentence = word_tokenize(sentence, format="text")
        # G√°n nh√£n t·ª´ lo·∫°i
        pos_tags = pos_tag(sentence)
        return pos_tags
    except Exception as e:
        print(f"‚ùå L·ªói khi g√°n nh√£n t·ª´ lo·∫°i: {e}")
        return None

# --------------------
# 3. H√†m k·∫øt h·ª£p chu·∫©n h√≥a v√† g√°n nh√£n t·ª´ lo·∫°i
# --------------------
def process_sentence(text):
    print(f"üìù Input g·ªëc: {text}")
    
    # Chu·∫©n h√≥a vƒÉn b·∫£n
    normalized_text = chuan_hoa_van_ban(text)
    print(f"‚úÖ VƒÉn b·∫£n chu·∫©n h√≥a: {normalized_text if normalized_text else text}")
    
    # G√°n nh√£n t·ª´ lo·∫°i
    pos_tags = gan_nhan_tu_loai(normalized_text if normalized_text else text)
    print(f"‚úÖ Nh√£n t·ª´ lo·∫°i: {pos_tags}")
    return normalized_text, pos_tags

# --------------------
# 4. Ch·∫°y th·ª≠ v·ªõi c√°c c√¢u
# --------------------
if __name__ == "__main__":
    test_sentences = [
        "T√¥i ƒëang h·ªçc NLP b·∫±ng ti·∫øng Vi·ªát.",  # C√¢u ƒë√£ chu·∫©n
        "mik k hieu j het",                  # C√¢u c·∫ßn chu·∫©n h√≥a
        "t ko bik j lun",
        "cx muon di chs nhug k co time"
    ]

    for sentence in test_sentences:
        print("\n" + "="*50)
        normalized, pos_tags = process_sentence(sentence)
        print("="*50 + "\n")